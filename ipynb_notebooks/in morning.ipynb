{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def save_sparse_csr(filename,array):\n",
    "    np.savez(filename,data = array.data ,indices=array.indices,\n",
    "             indptr =array.indptr, shape=array.shape )\n",
    "\n",
    "def load_sparse_csr(filename):\n",
    "    loader = np.load(filename)\n",
    "    return csr_matrix((  loader['data'], loader['indices'], loader['indptr']),\n",
    "                         shape = loader['shape'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datadir = 'input/'\n",
    "gatrain = pd.read_csv(os.path.join(datadir,'gender_age_train.csv'),\n",
    "                      index_col='device_id')\n",
    "gatest = pd.read_csv(os.path.join(datadir,'gender_age_test.csv'),\n",
    "                     index_col = 'device_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_tr_app_events_sparse = load_sparse_csr('data/df_tr_app_events.npz')\n",
    "rows1tr = pd.read_csv('data/rows1tr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tr_app_events = pd.DataFrame(df_tr_app_events_sparse.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tr_app_events.index = rows1tr['device_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#118724\n",
    "a = pd.Series(df_tr_app_events.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9623    118724.010261\n",
       "dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>118724]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_tr_app_events.drop([9623],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12476, 10115)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr_app_events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "targetencoder = LabelEncoder().fit(gatrain.ix[df_tr_app_events.index].group)\n",
    "y = targetencoder.transform(gatrain.ix[df_tr_app_events.index].group)\n",
    "nclasses = len(targetencoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(clf, random_state = 24):\n",
    "    kf = StratifiedKFold(y, n_folds=10, shuffle=True, random_state=random_state)\n",
    "    pred = np.zeros((y.shape[0],nclasses))\n",
    "    for itrain, itest in kf:\n",
    "        Xtr, Xte = Xtrain[itrain, :], Xtrain[itest, :]\n",
    "        ytr, yte = y[itrain], y[itest]\n",
    "        clf.fit(Xtr, ytr)\n",
    "        pred[itest,:] = clf.predict_proba(Xte)\n",
    "        # Downsize to one fold only for kernels\n",
    "        print(\"{:.5f}\".format(log_loss(yte, pred[itest,:])), end=' ')\n",
    "        return log_loss(yte, pred[itest, :])\n",
    "    print(\"{:.5f}\".format(log_loss(y, pred)), end=' ')\n",
    "    return log_loss(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Cs = np.logspace(-3,0,10)\n",
    "print(Cs)\n",
    "res = []\n",
    "for C in Cs:\n",
    "    res.append(score(LogisticRegression(C = C, multi_class='multinomial',solver='lbfgs')))\n",
    "plt.semilogx(Cs, res,'-o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(csr_matrix(df_tr_app_events), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "        \"eta\": 0.3,\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"max_depth\": 3,\n",
    "        \"gamma\": 0.3,\n",
    "        \"subsample\": 0.7,\n",
    "        \"colsample_bytree\": 0.7,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": 1233,\n",
    "        \"num_class\": 12,\n",
    "        \"nthread\": 16,\n",
    "        \"eval_metric\": \"mlogloss\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.38551+0.00212221\ttest-mlogloss:2.4024+0.00313538\n",
      "[10]\ttrain-mlogloss:2.03947+0.00215676\ttest-mlogloss:2.1716+0.00632376\n",
      "[20]\ttrain-mlogloss:1.89973+0.0032087\ttest-mlogloss:2.11897+0.00828652\n",
      "[30]\ttrain-mlogloss:1.8017+0.00274795\ttest-mlogloss:2.09854+0.00866225\n",
      "[40]\ttrain-mlogloss:1.72295+0.0031276\ttest-mlogloss:2.08526+0.00764471\n",
      "[50]\ttrain-mlogloss:1.6563+0.00450869\ttest-mlogloss:2.07734+0.00782482\n",
      "[60]\ttrain-mlogloss:1.5964+0.00514728\ttest-mlogloss:2.07232+0.00980584\n",
      "[70]\ttrain-mlogloss:1.54302+0.00407783\ttest-mlogloss:2.06805+0.0112398\n",
      "[80]\ttrain-mlogloss:1.49336+0.00461083\ttest-mlogloss:2.06584+0.0119663\n",
      "[90]\ttrain-mlogloss:1.44853+0.00507031\ttest-mlogloss:2.0646+0.0123073\n",
      "[100]\ttrain-mlogloss:1.40719+0.0059264\ttest-mlogloss:2.06556+0.0144705\n",
      "[110]\ttrain-mlogloss:1.36823+0.00584503\ttest-mlogloss:2.06605+0.0147907\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-mlogloss-mean</th>\n",
       "      <th>test-mlogloss-std</th>\n",
       "      <th>train-mlogloss-mean</th>\n",
       "      <th>train-mlogloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.402401</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>2.385509</td>\n",
       "      <td>0.002122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.345507</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>2.315201</td>\n",
       "      <td>0.002170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.305561</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>2.261820</td>\n",
       "      <td>0.002154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.275109</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>2.218485</td>\n",
       "      <td>0.001232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.250508</td>\n",
       "      <td>0.006138</td>\n",
       "      <td>2.182705</td>\n",
       "      <td>0.001377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.231612</td>\n",
       "      <td>0.006477</td>\n",
       "      <td>2.152241</td>\n",
       "      <td>0.000941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.214663</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>2.124854</td>\n",
       "      <td>0.001222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.201581</td>\n",
       "      <td>0.007883</td>\n",
       "      <td>2.100704</td>\n",
       "      <td>0.001443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.190276</td>\n",
       "      <td>0.007477</td>\n",
       "      <td>2.078551</td>\n",
       "      <td>0.001751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.179991</td>\n",
       "      <td>0.006856</td>\n",
       "      <td>2.058141</td>\n",
       "      <td>0.001972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.171605</td>\n",
       "      <td>0.006324</td>\n",
       "      <td>2.039471</td>\n",
       "      <td>0.002157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.163975</td>\n",
       "      <td>0.007643</td>\n",
       "      <td>2.022176</td>\n",
       "      <td>0.002570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.156888</td>\n",
       "      <td>0.007650</td>\n",
       "      <td>2.005742</td>\n",
       "      <td>0.002675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.150884</td>\n",
       "      <td>0.007944</td>\n",
       "      <td>1.990156</td>\n",
       "      <td>0.002877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.144408</td>\n",
       "      <td>0.008470</td>\n",
       "      <td>1.975601</td>\n",
       "      <td>0.003381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.139129</td>\n",
       "      <td>0.007843</td>\n",
       "      <td>1.961204</td>\n",
       "      <td>0.003252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.134764</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>1.948082</td>\n",
       "      <td>0.002800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.130088</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>1.934940</td>\n",
       "      <td>0.002674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.126459</td>\n",
       "      <td>0.008183</td>\n",
       "      <td>1.922904</td>\n",
       "      <td>0.002571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.123181</td>\n",
       "      <td>0.008068</td>\n",
       "      <td>1.911415</td>\n",
       "      <td>0.002716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.118966</td>\n",
       "      <td>0.008287</td>\n",
       "      <td>1.899735</td>\n",
       "      <td>0.003209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.115558</td>\n",
       "      <td>0.007793</td>\n",
       "      <td>1.888622</td>\n",
       "      <td>0.002941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.113322</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>1.877977</td>\n",
       "      <td>0.002598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.111290</td>\n",
       "      <td>0.007908</td>\n",
       "      <td>1.867681</td>\n",
       "      <td>0.002061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.109578</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>1.857803</td>\n",
       "      <td>0.002077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.106888</td>\n",
       "      <td>0.008623</td>\n",
       "      <td>1.847612</td>\n",
       "      <td>0.001997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.104734</td>\n",
       "      <td>0.008663</td>\n",
       "      <td>1.837821</td>\n",
       "      <td>0.002155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.103292</td>\n",
       "      <td>0.008792</td>\n",
       "      <td>1.828608</td>\n",
       "      <td>0.002606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.101828</td>\n",
       "      <td>0.008674</td>\n",
       "      <td>1.819491</td>\n",
       "      <td>0.002867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.100028</td>\n",
       "      <td>0.008973</td>\n",
       "      <td>1.810496</td>\n",
       "      <td>0.002469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2.072082</td>\n",
       "      <td>0.009851</td>\n",
       "      <td>1.585253</td>\n",
       "      <td>0.005224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>2.071400</td>\n",
       "      <td>0.009796</td>\n",
       "      <td>1.579873</td>\n",
       "      <td>0.004890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>2.070887</td>\n",
       "      <td>0.009331</td>\n",
       "      <td>1.574401</td>\n",
       "      <td>0.004929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>2.070553</td>\n",
       "      <td>0.009454</td>\n",
       "      <td>1.569339</td>\n",
       "      <td>0.004825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>2.070504</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>1.563830</td>\n",
       "      <td>0.004788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2.069821</td>\n",
       "      <td>0.010458</td>\n",
       "      <td>1.558622</td>\n",
       "      <td>0.004636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2.069261</td>\n",
       "      <td>0.010698</td>\n",
       "      <td>1.553400</td>\n",
       "      <td>0.004347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2.068734</td>\n",
       "      <td>0.010525</td>\n",
       "      <td>1.548108</td>\n",
       "      <td>0.004113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2.068053</td>\n",
       "      <td>0.011240</td>\n",
       "      <td>1.543024</td>\n",
       "      <td>0.004078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2.067750</td>\n",
       "      <td>0.011270</td>\n",
       "      <td>1.537960</td>\n",
       "      <td>0.004337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2.067604</td>\n",
       "      <td>0.011210</td>\n",
       "      <td>1.532902</td>\n",
       "      <td>0.004208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>2.067305</td>\n",
       "      <td>0.010912</td>\n",
       "      <td>1.528089</td>\n",
       "      <td>0.004154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2.066957</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>1.523268</td>\n",
       "      <td>0.004191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2.067192</td>\n",
       "      <td>0.011418</td>\n",
       "      <td>1.518137</td>\n",
       "      <td>0.004426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>2.066986</td>\n",
       "      <td>0.011593</td>\n",
       "      <td>1.513055</td>\n",
       "      <td>0.004353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2.067156</td>\n",
       "      <td>0.012401</td>\n",
       "      <td>1.508391</td>\n",
       "      <td>0.004433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>2.066917</td>\n",
       "      <td>0.012233</td>\n",
       "      <td>1.503303</td>\n",
       "      <td>0.004395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>2.066099</td>\n",
       "      <td>0.012323</td>\n",
       "      <td>1.498162</td>\n",
       "      <td>0.004408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2.065836</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>1.493356</td>\n",
       "      <td>0.004611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>2.065535</td>\n",
       "      <td>0.011994</td>\n",
       "      <td>1.488897</td>\n",
       "      <td>0.004539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>2.065482</td>\n",
       "      <td>0.012381</td>\n",
       "      <td>1.484373</td>\n",
       "      <td>0.004789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>2.065911</td>\n",
       "      <td>0.012193</td>\n",
       "      <td>1.479440</td>\n",
       "      <td>0.004994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2.065327</td>\n",
       "      <td>0.011996</td>\n",
       "      <td>1.474885</td>\n",
       "      <td>0.005005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>2.065333</td>\n",
       "      <td>0.012189</td>\n",
       "      <td>1.470424</td>\n",
       "      <td>0.005054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>2.064834</td>\n",
       "      <td>0.012044</td>\n",
       "      <td>1.465978</td>\n",
       "      <td>0.005074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>2.064945</td>\n",
       "      <td>0.012119</td>\n",
       "      <td>1.461606</td>\n",
       "      <td>0.004754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>2.064784</td>\n",
       "      <td>0.011892</td>\n",
       "      <td>1.457303</td>\n",
       "      <td>0.004926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>2.065080</td>\n",
       "      <td>0.011965</td>\n",
       "      <td>1.452795</td>\n",
       "      <td>0.005071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>2.064597</td>\n",
       "      <td>0.012307</td>\n",
       "      <td>1.448534</td>\n",
       "      <td>0.005070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2.064409</td>\n",
       "      <td>0.012519</td>\n",
       "      <td>1.444201</td>\n",
       "      <td>0.005057</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>92 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    test-mlogloss-mean  test-mlogloss-std  train-mlogloss-mean  \\\n",
       "0             2.402401           0.003135             2.385509   \n",
       "1             2.345507           0.003686             2.315201   \n",
       "2             2.305561           0.004415             2.261820   \n",
       "3             2.275109           0.004451             2.218485   \n",
       "4             2.250508           0.006138             2.182705   \n",
       "5             2.231612           0.006477             2.152241   \n",
       "6             2.214663           0.007932             2.124854   \n",
       "7             2.201581           0.007883             2.100704   \n",
       "8             2.190276           0.007477             2.078551   \n",
       "9             2.179991           0.006856             2.058141   \n",
       "10            2.171605           0.006324             2.039471   \n",
       "11            2.163975           0.007643             2.022176   \n",
       "12            2.156888           0.007650             2.005742   \n",
       "13            2.150884           0.007944             1.990156   \n",
       "14            2.144408           0.008470             1.975601   \n",
       "15            2.139129           0.007843             1.961204   \n",
       "16            2.134764           0.007875             1.948082   \n",
       "17            2.130088           0.008151             1.934940   \n",
       "18            2.126459           0.008183             1.922904   \n",
       "19            2.123181           0.008068             1.911415   \n",
       "20            2.118966           0.008287             1.899735   \n",
       "21            2.115558           0.007793             1.888622   \n",
       "22            2.113322           0.007642             1.877977   \n",
       "23            2.111290           0.007908             1.867681   \n",
       "24            2.109578           0.008095             1.857803   \n",
       "25            2.106888           0.008623             1.847612   \n",
       "26            2.104734           0.008663             1.837821   \n",
       "27            2.103292           0.008792             1.828608   \n",
       "28            2.101828           0.008674             1.819491   \n",
       "29            2.100028           0.008973             1.810496   \n",
       "..                 ...                ...                  ...   \n",
       "62            2.072082           0.009851             1.585253   \n",
       "63            2.071400           0.009796             1.579873   \n",
       "64            2.070887           0.009331             1.574401   \n",
       "65            2.070553           0.009454             1.569339   \n",
       "66            2.070504           0.009598             1.563830   \n",
       "67            2.069821           0.010458             1.558622   \n",
       "68            2.069261           0.010698             1.553400   \n",
       "69            2.068734           0.010525             1.548108   \n",
       "70            2.068053           0.011240             1.543024   \n",
       "71            2.067750           0.011270             1.537960   \n",
       "72            2.067604           0.011210             1.532902   \n",
       "73            2.067305           0.010912             1.528089   \n",
       "74            2.066957           0.010594             1.523268   \n",
       "75            2.067192           0.011418             1.518137   \n",
       "76            2.066986           0.011593             1.513055   \n",
       "77            2.067156           0.012401             1.508391   \n",
       "78            2.066917           0.012233             1.503303   \n",
       "79            2.066099           0.012323             1.498162   \n",
       "80            2.065836           0.011966             1.493356   \n",
       "81            2.065535           0.011994             1.488897   \n",
       "82            2.065482           0.012381             1.484373   \n",
       "83            2.065911           0.012193             1.479440   \n",
       "84            2.065327           0.011996             1.474885   \n",
       "85            2.065333           0.012189             1.470424   \n",
       "86            2.064834           0.012044             1.465978   \n",
       "87            2.064945           0.012119             1.461606   \n",
       "88            2.064784           0.011892             1.457303   \n",
       "89            2.065080           0.011965             1.452795   \n",
       "90            2.064597           0.012307             1.448534   \n",
       "91            2.064409           0.012519             1.444201   \n",
       "\n",
       "    train-mlogloss-std  \n",
       "0             0.002122  \n",
       "1             0.002170  \n",
       "2             0.002154  \n",
       "3             0.001232  \n",
       "4             0.001377  \n",
       "5             0.000941  \n",
       "6             0.001222  \n",
       "7             0.001443  \n",
       "8             0.001751  \n",
       "9             0.001972  \n",
       "10            0.002157  \n",
       "11            0.002570  \n",
       "12            0.002675  \n",
       "13            0.002877  \n",
       "14            0.003381  \n",
       "15            0.003252  \n",
       "16            0.002800  \n",
       "17            0.002674  \n",
       "18            0.002571  \n",
       "19            0.002716  \n",
       "20            0.003209  \n",
       "21            0.002941  \n",
       "22            0.002598  \n",
       "23            0.002061  \n",
       "24            0.002077  \n",
       "25            0.001997  \n",
       "26            0.002155  \n",
       "27            0.002606  \n",
       "28            0.002867  \n",
       "29            0.002469  \n",
       "..                 ...  \n",
       "62            0.005224  \n",
       "63            0.004890  \n",
       "64            0.004929  \n",
       "65            0.004825  \n",
       "66            0.004788  \n",
       "67            0.004636  \n",
       "68            0.004347  \n",
       "69            0.004113  \n",
       "70            0.004078  \n",
       "71            0.004337  \n",
       "72            0.004208  \n",
       "73            0.004154  \n",
       "74            0.004191  \n",
       "75            0.004426  \n",
       "76            0.004353  \n",
       "77            0.004433  \n",
       "78            0.004395  \n",
       "79            0.004408  \n",
       "80            0.004611  \n",
       "81            0.004539  \n",
       "82            0.004789  \n",
       "83            0.004994  \n",
       "84            0.005005  \n",
       "85            0.005054  \n",
       "86            0.005074  \n",
       "87            0.004754  \n",
       "88            0.004926  \n",
       "89            0.005071  \n",
       "90            0.005070  \n",
       "91            0.005057  \n",
       "\n",
       "[92 rows x 4 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.cv(params, dtrain, \n",
    "       num_boost_round=10000, \n",
    "       early_stopping_rounds = 20, \n",
    "       maximize = False,\n",
    "       nfold=5,\n",
    "       stratified=True,\n",
    "       verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.04208 2.02506 2.03599 2.01712 2.00233 2.01351 2.01541 2.03365 2.02277 2.03831 2.02462 2.03400 2.01593 2.02786 2.00919 1.99293 2.00518 2.00676 2.02544 2.01409 2.03218 2.01635 2.02738 2.01025 2.02144 2.00378 1.98523 1.99974 2.00085 2.02008 2.00804 2.02701 2.01038 2.02395 2.00701 2.01742 2.00187 1.98088 1.99632 1.99623 2.01729 2.00406 2.02442 2.00694 2.02248 2.00610 2.01563 2.00215 1.97832 1.99566 1.99387 2.01698 2.00304 2.02444 2.00586 2.02357 2.00758 2.01645 2.00567 1.97833 1.99810 1.99476 2.01929 2.00459 2.02550 2.00738 2.02670 2.01345 2.01958 2.01064 1.98042 2.00252 1.99811 2.02466 2.01008 2.02993 2.01160 2.03461 2.02216 2.02487 2.02130 1.98605 2.00807 2.00443 2.03668 2.02061 2.03831 2.01970 2.04437 2.03111 2.03752 2.03556 1.99077 2.02932 2.01304 2.04374 2.03099 2.04722 2.03036 2.06670 2.04084 2.05142 2.05176 2.00599 2.04072 2.02782 2.06491 2.04717 2.06256 2.04598 "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEECAYAAAAlEzNMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f94e9c1be48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Cs = np.logspace(-2,-1,10)\n",
    "res = []\n",
    "for C in Cs:\n",
    "    res.append(score(LogisticRegression(C = C, multi_class='multinomial',solver='lbfgs')))\n",
    "plt.semilogx(Cs, res,'-o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01      ,  0.0129155 ,  0.01668101,  0.02154435,  0.02782559,\n",
       "        0.03593814,  0.04641589,  0.05994843,  0.07742637,  0.1       ])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logspace(-2,-1,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.00386 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.0038583531682344"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0.02782559\n",
    "score(LogisticRegression(C = 0.02782559, multi_class='multinomial',solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, n_components=0.999, whiten=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.999)\n",
    "pca.fit(df_tr_app_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3111\n"
     ]
    }
   ],
   "source": [
    "print(pca.n_components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain_new = pca.transform(df_tr_app_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain = csr_matrix(Xtrain_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12476, 3111)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.02335 2.00637 2.01538 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-0850eb25b7e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.02782559\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'multinomial'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msolver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-80-0118acb7ab65>\u001b[0m in \u001b[0;36mscore\u001b[1;34m(clf, random_state)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXte\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mytr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myte\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXtr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mpred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Downsize to one fold only for kernels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1205\u001b[0m                       \u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_squared_sum\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1206\u001b[0m                       sample_weight=sample_weight)\n\u001b[1;32m-> 1207\u001b[1;33m             for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))\n\u001b[0m\u001b[0;32m   1208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1209\u001b[0m         \u001b[0mfold_coefs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfold_coefs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    656\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 566\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateComputeBatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    567\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mlogistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, copy, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight)\u001b[0m\n\u001b[0;32m    691\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfprime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                     \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m                     iprint=(verbose > 0) - 1, pgtol=tol, maxiter=max_iter)\n\u001b[0m\u001b[0;32m    694\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m                 \u001b[1;31m# old scipy doesn't have maxiter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfmin_l_bfgs_b\u001b[1;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     res = _minimize_lbfgsb(fun, x0, args=args, jac=jac, bounds=bounds,\n\u001b[1;32m--> 193\u001b[1;33m                            **opts)\n\u001b[0m\u001b[0;32m    194\u001b[0m     d = {'grad': res['jac'],\n\u001b[0;32m    195\u001b[0m          \u001b[1;34m'task'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'message'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[0;32m    328\u001b[0m                 \u001b[1;31m# minimization routine wants f and g at the current x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                 \u001b[1;31m# Overwrite f and g:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb'NEW_X'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[1;31m# new iteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    276\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    277\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    279\u001b[0m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[1;34m(*wrapper_args)\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 289\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m         \u001b[0mfg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x, *args)\u001b[0m\n\u001b[0;32m    668\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_binarized\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'lbfgs'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_multinomial_loss_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0msolver\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss_grad\u001b[1;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[0mfit_intercept\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mn_classes\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mn_features\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_multinomial_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[0msample_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[0mdiff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36m_multinomial_loss\u001b[1;34m(w, X, Y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m         \u001b[0mintercept\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mintercept\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    177\u001b[0m     \"\"\"\n\u001b[0;32m    178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdense_output\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"toarray\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m             \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    308\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mul_multivector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misscalarlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/Lenovo/anaconda3/lib/python3.5/site-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m_mul_multivector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;31m# csr_matvecs or csc_matvecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    468\u001b[0m         \u001b[0mfn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sparsetools\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_matvecs'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m         \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_vecs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "score(LogisticRegression(C = 0.02782559, multi_class='multinomial',solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "dtrain = xgb.DMatrix(X_new, y)\n",
    "params = {\n",
    "        \"eta\": 0.3,\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"objective\": \"multi:softprob\",\n",
    "        \"max_depth\": 5,\n",
    "        \"silent\": 1,\n",
    "        \"seed\": 1233,\n",
    "        \"num_class\": 12,\n",
    "        \"nthread\": 16,\n",
    "        \"eval_metric\": \"mlogloss\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:2.33506+0.000854135\ttest-mlogloss:2.38536+0.00530543\n",
      "[10]\ttrain-mlogloss:1.80025+0.0041391\ttest-mlogloss:2.13942+0.0164785\n",
      "[20]\ttrain-mlogloss:1.58663+0.00620401\ttest-mlogloss:2.09194+0.0226143\n",
      "[30]\ttrain-mlogloss:1.45792+0.00672583\ttest-mlogloss:2.07605+0.0265523\n",
      "[40]\ttrain-mlogloss:1.36162+0.00513704\ttest-mlogloss:2.06868+0.0293274\n",
      "[50]\ttrain-mlogloss:1.28191+0.0052866\ttest-mlogloss:2.06792+0.0298315\n",
      "[60]\ttrain-mlogloss:1.21357+0.00565554\ttest-mlogloss:2.06856+0.0301992\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test-mlogloss-mean</th>\n",
       "      <th>test-mlogloss-std</th>\n",
       "      <th>train-mlogloss-mean</th>\n",
       "      <th>train-mlogloss-std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.385356</td>\n",
       "      <td>0.005305</td>\n",
       "      <td>2.335062</td>\n",
       "      <td>0.000854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.323365</td>\n",
       "      <td>0.008254</td>\n",
       "      <td>2.232456</td>\n",
       "      <td>0.001718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.280256</td>\n",
       "      <td>0.009369</td>\n",
       "      <td>2.153086</td>\n",
       "      <td>0.002202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.248039</td>\n",
       "      <td>0.010095</td>\n",
       "      <td>2.087226</td>\n",
       "      <td>0.002635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.222386</td>\n",
       "      <td>0.013361</td>\n",
       "      <td>2.031566</td>\n",
       "      <td>0.002920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.201524</td>\n",
       "      <td>0.013438</td>\n",
       "      <td>1.983009</td>\n",
       "      <td>0.002578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.185329</td>\n",
       "      <td>0.014820</td>\n",
       "      <td>1.938991</td>\n",
       "      <td>0.002442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.170451</td>\n",
       "      <td>0.014408</td>\n",
       "      <td>1.899712</td>\n",
       "      <td>0.003184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.157886</td>\n",
       "      <td>0.015476</td>\n",
       "      <td>1.863756</td>\n",
       "      <td>0.003629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.147742</td>\n",
       "      <td>0.015624</td>\n",
       "      <td>1.831014</td>\n",
       "      <td>0.003462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.139416</td>\n",
       "      <td>0.016479</td>\n",
       "      <td>1.800252</td>\n",
       "      <td>0.004139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.131973</td>\n",
       "      <td>0.017277</td>\n",
       "      <td>1.771948</td>\n",
       "      <td>0.003683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.124705</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>1.745191</td>\n",
       "      <td>0.004073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.118520</td>\n",
       "      <td>0.019508</td>\n",
       "      <td>1.720248</td>\n",
       "      <td>0.004613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2.113396</td>\n",
       "      <td>0.019790</td>\n",
       "      <td>1.697772</td>\n",
       "      <td>0.004718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2.108755</td>\n",
       "      <td>0.020471</td>\n",
       "      <td>1.676565</td>\n",
       "      <td>0.005791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.104541</td>\n",
       "      <td>0.020909</td>\n",
       "      <td>1.656834</td>\n",
       "      <td>0.005407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2.100499</td>\n",
       "      <td>0.021823</td>\n",
       "      <td>1.637836</td>\n",
       "      <td>0.005162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.096634</td>\n",
       "      <td>0.021686</td>\n",
       "      <td>1.619918</td>\n",
       "      <td>0.005662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2.094138</td>\n",
       "      <td>0.022334</td>\n",
       "      <td>1.602933</td>\n",
       "      <td>0.006198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.091945</td>\n",
       "      <td>0.022614</td>\n",
       "      <td>1.586630</td>\n",
       "      <td>0.006204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2.089333</td>\n",
       "      <td>0.023152</td>\n",
       "      <td>1.571477</td>\n",
       "      <td>0.005105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.086581</td>\n",
       "      <td>0.022986</td>\n",
       "      <td>1.556855</td>\n",
       "      <td>0.005737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2.084798</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>1.542955</td>\n",
       "      <td>0.005623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2.082498</td>\n",
       "      <td>0.023936</td>\n",
       "      <td>1.529923</td>\n",
       "      <td>0.005532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2.081527</td>\n",
       "      <td>0.024468</td>\n",
       "      <td>1.516762</td>\n",
       "      <td>0.005740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2.080381</td>\n",
       "      <td>0.025051</td>\n",
       "      <td>1.504198</td>\n",
       "      <td>0.006327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2.079058</td>\n",
       "      <td>0.024724</td>\n",
       "      <td>1.492673</td>\n",
       "      <td>0.006498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2.077532</td>\n",
       "      <td>0.025798</td>\n",
       "      <td>1.480328</td>\n",
       "      <td>0.006944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.076751</td>\n",
       "      <td>0.025968</td>\n",
       "      <td>1.469096</td>\n",
       "      <td>0.006630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.076047</td>\n",
       "      <td>0.026552</td>\n",
       "      <td>1.457917</td>\n",
       "      <td>0.006726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2.075320</td>\n",
       "      <td>0.026329</td>\n",
       "      <td>1.446907</td>\n",
       "      <td>0.006618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2.073996</td>\n",
       "      <td>0.027289</td>\n",
       "      <td>1.435909</td>\n",
       "      <td>0.007060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2.073564</td>\n",
       "      <td>0.027312</td>\n",
       "      <td>1.425866</td>\n",
       "      <td>0.006336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2.072359</td>\n",
       "      <td>0.027751</td>\n",
       "      <td>1.416268</td>\n",
       "      <td>0.006235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2.071780</td>\n",
       "      <td>0.027739</td>\n",
       "      <td>1.406712</td>\n",
       "      <td>0.005670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.071315</td>\n",
       "      <td>0.028071</td>\n",
       "      <td>1.397307</td>\n",
       "      <td>0.006343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2.070777</td>\n",
       "      <td>0.028325</td>\n",
       "      <td>1.387540</td>\n",
       "      <td>0.006763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2.069998</td>\n",
       "      <td>0.028879</td>\n",
       "      <td>1.378386</td>\n",
       "      <td>0.005951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.069219</td>\n",
       "      <td>0.029353</td>\n",
       "      <td>1.370285</td>\n",
       "      <td>0.005480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2.068684</td>\n",
       "      <td>0.029327</td>\n",
       "      <td>1.361625</td>\n",
       "      <td>0.005137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2.068645</td>\n",
       "      <td>0.029649</td>\n",
       "      <td>1.353361</td>\n",
       "      <td>0.005039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.068852</td>\n",
       "      <td>0.029860</td>\n",
       "      <td>1.344417</td>\n",
       "      <td>0.004723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.068560</td>\n",
       "      <td>0.029852</td>\n",
       "      <td>1.336271</td>\n",
       "      <td>0.005118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2.068715</td>\n",
       "      <td>0.029555</td>\n",
       "      <td>1.328202</td>\n",
       "      <td>0.004951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2.068746</td>\n",
       "      <td>0.029092</td>\n",
       "      <td>1.320676</td>\n",
       "      <td>0.005053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2.068615</td>\n",
       "      <td>0.028798</td>\n",
       "      <td>1.312909</td>\n",
       "      <td>0.005090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2.068479</td>\n",
       "      <td>0.029228</td>\n",
       "      <td>1.304989</td>\n",
       "      <td>0.005008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2.068398</td>\n",
       "      <td>0.029521</td>\n",
       "      <td>1.297218</td>\n",
       "      <td>0.005434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2.068186</td>\n",
       "      <td>0.029555</td>\n",
       "      <td>1.289271</td>\n",
       "      <td>0.005573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2.067917</td>\n",
       "      <td>0.029832</td>\n",
       "      <td>1.281910</td>\n",
       "      <td>0.005287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test-mlogloss-mean  test-mlogloss-std  train-mlogloss-mean  \\\n",
       "0             2.385356           0.005305             2.335062   \n",
       "1             2.323365           0.008254             2.232456   \n",
       "2             2.280256           0.009369             2.153086   \n",
       "3             2.248039           0.010095             2.087226   \n",
       "4             2.222386           0.013361             2.031566   \n",
       "5             2.201524           0.013438             1.983009   \n",
       "6             2.185329           0.014820             1.938991   \n",
       "7             2.170451           0.014408             1.899712   \n",
       "8             2.157886           0.015476             1.863756   \n",
       "9             2.147742           0.015624             1.831014   \n",
       "10            2.139416           0.016479             1.800252   \n",
       "11            2.131973           0.017277             1.771948   \n",
       "12            2.124705           0.018160             1.745191   \n",
       "13            2.118520           0.019508             1.720248   \n",
       "14            2.113396           0.019790             1.697772   \n",
       "15            2.108755           0.020471             1.676565   \n",
       "16            2.104541           0.020909             1.656834   \n",
       "17            2.100499           0.021823             1.637836   \n",
       "18            2.096634           0.021686             1.619918   \n",
       "19            2.094138           0.022334             1.602933   \n",
       "20            2.091945           0.022614             1.586630   \n",
       "21            2.089333           0.023152             1.571477   \n",
       "22            2.086581           0.022986             1.556855   \n",
       "23            2.084798           0.023438             1.542955   \n",
       "24            2.082498           0.023936             1.529923   \n",
       "25            2.081527           0.024468             1.516762   \n",
       "26            2.080381           0.025051             1.504198   \n",
       "27            2.079058           0.024724             1.492673   \n",
       "28            2.077532           0.025798             1.480328   \n",
       "29            2.076751           0.025968             1.469096   \n",
       "30            2.076047           0.026552             1.457917   \n",
       "31            2.075320           0.026329             1.446907   \n",
       "32            2.073996           0.027289             1.435909   \n",
       "33            2.073564           0.027312             1.425866   \n",
       "34            2.072359           0.027751             1.416268   \n",
       "35            2.071780           0.027739             1.406712   \n",
       "36            2.071315           0.028071             1.397307   \n",
       "37            2.070777           0.028325             1.387540   \n",
       "38            2.069998           0.028879             1.378386   \n",
       "39            2.069219           0.029353             1.370285   \n",
       "40            2.068684           0.029327             1.361625   \n",
       "41            2.068645           0.029649             1.353361   \n",
       "42            2.068852           0.029860             1.344417   \n",
       "43            2.068560           0.029852             1.336271   \n",
       "44            2.068715           0.029555             1.328202   \n",
       "45            2.068746           0.029092             1.320676   \n",
       "46            2.068615           0.028798             1.312909   \n",
       "47            2.068479           0.029228             1.304989   \n",
       "48            2.068398           0.029521             1.297218   \n",
       "49            2.068186           0.029555             1.289271   \n",
       "50            2.067917           0.029832             1.281910   \n",
       "\n",
       "    train-mlogloss-std  \n",
       "0             0.000854  \n",
       "1             0.001718  \n",
       "2             0.002202  \n",
       "3             0.002635  \n",
       "4             0.002920  \n",
       "5             0.002578  \n",
       "6             0.002442  \n",
       "7             0.003184  \n",
       "8             0.003629  \n",
       "9             0.003462  \n",
       "10            0.004139  \n",
       "11            0.003683  \n",
       "12            0.004073  \n",
       "13            0.004613  \n",
       "14            0.004718  \n",
       "15            0.005791  \n",
       "16            0.005407  \n",
       "17            0.005162  \n",
       "18            0.005662  \n",
       "19            0.006198  \n",
       "20            0.006204  \n",
       "21            0.005105  \n",
       "22            0.005737  \n",
       "23            0.005623  \n",
       "24            0.005532  \n",
       "25            0.005740  \n",
       "26            0.006327  \n",
       "27            0.006498  \n",
       "28            0.006944  \n",
       "29            0.006630  \n",
       "30            0.006726  \n",
       "31            0.006618  \n",
       "32            0.007060  \n",
       "33            0.006336  \n",
       "34            0.006235  \n",
       "35            0.005670  \n",
       "36            0.006343  \n",
       "37            0.006763  \n",
       "38            0.005951  \n",
       "39            0.005480  \n",
       "40            0.005137  \n",
       "41            0.005039  \n",
       "42            0.004723  \n",
       "43            0.005118  \n",
       "44            0.004951  \n",
       "45            0.005053  \n",
       "46            0.005090  \n",
       "47            0.005008  \n",
       "48            0.005434  \n",
       "49            0.005573  \n",
       "50            0.005287  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.cv(params, dtrain, \n",
    "       num_boost_round=10000, \n",
    "       early_stopping_rounds = 20, \n",
    "       maximize = False,\n",
    "       nfold=10,\n",
    "       stratified=True,\n",
    "       verbose_eval=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12476, 10115)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr_app_events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain = csr_matrix(df_tr_app_events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.2         0.21111111  0.22222222  0.23333333  0.24444444  0.25555556\n",
      "  0.26666667  0.27777778  0.28888889  0.3       ]\n",
      "2.01319 2.01278 2.01253 2.01232 2.01223 2.01222 2.01250 2.01305 2.01405 2.01493 "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEGCAYAAACdJRn3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nOu3/ABSjSKP4aFKdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f957f282ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Cs = np.linspace(0.2,0.3,10)\n",
    "print(Cs)\n",
    "res = []\n",
    "for C in Cs:\n",
    "    res.append(score(LogisticRegression(C = C, penalty=\"l1\", dual=False)))\n",
    "plt.plot(Cs, res,'-o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "clf = LogisticRegression(C = 0.23333333, penalty=\"l1\", dual=False).fit(Xtrain, y)\n",
    "model = SelectFromModel(clf, prefit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new = model.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12476, 656)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_sparse_csr('selectfrommodel_l1_logistic',X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y.to_csv('Y.csv',index_label='index',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
